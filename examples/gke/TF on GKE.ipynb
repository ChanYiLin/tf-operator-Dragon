{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF on GKE\n",
    "\n",
    "This notebook shows how to run the [TensorFlow CIFAR10 sample](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator) on GKE using [TFJobs](https://github.com/kubeflow/tf-operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you must have the following installed\n",
    "  * gcloud\n",
    "  * kubectl\n",
    "  * helm\n",
    "  * kubernetes python client library\n",
    "  \n",
    "There is a Docker image based on Datalab suitable for running this notebook.\n",
    "\n",
    "You can start that container as follows\n",
    "\n",
    "```\n",
    "docker run --name=gke-datalab -p \"127.0.0.1:8081:8080\" \\\n",
    "    -v \"${HOME}:/content/datalab/home\" \\\n",
    "    -v /var/run/docker.sock:/var/run/docker.sock -d  -e \"PROJECT_ID=\" \\\n",
    "    gcr.io/tf-on-k8s-dogfood/gke-datalab:v20171103-73616f0\n",
    "```\n",
    "  * You need to map in docker if you want tobuild docker images inside the container.\n",
    "  * Alternatively, you can set \"use_gcb\" to true in order to build the images using Google Container Builder\n",
    "  \n",
    "Additionally the [py package](https://github.com/kubeflow/tf-operator/tree/master/py) must be a top level package importable as py\n",
    "  * If you cloned [kubeflow/tf-operator](https://github.com/kubeflow/tf-operator) and are running this notebook in place the path with be configured automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Turn on autoreloading\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import a bunch of modules and set some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Assumes we are running inside the cloned repo.\n",
    "# Try to setup the path so we can import py as a top level package\n",
    "ROOT_DIR = os.path.abspath(os.path.join(\"../..\"))\n",
    "if os.path.exists(os.path.join(ROOT_DIR, \"py\")):\n",
    "  if not ROOT_DIR in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "  \n",
    "import kubernetes\n",
    "from kubernetes import client as k8s_client\n",
    "from kubernetes import config as k8s_config\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubernetes.client.models.v1_label_selector import V1LabelSelector\n",
    "import datetime\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from pprint import pprint\n",
    "try:\n",
    "  from py import build_and_push_image\n",
    "  from py import util\n",
    "except ImportError:\n",
    "  raise ImportError(\"Please ensure the py package in https://github.com/kubeflow/tf-operator is a top level package\")\n",
    "import StringIO\n",
    "import subprocess\n",
    "import urllib\n",
    "import urllib2\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "TF_JOB_GROUP = \"kubeflow.org\"\n",
    "TF_JOB_VERSION = \"v1alpha1\"\n",
    "TF_JOB_PLURAL = \"tfjobs\"\n",
    "TF_JOB_KIND = \"TFJob\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the notebook for your use\n",
    "Change the constants defined below.\n",
    "  1. Change **project** to a project you have access to.\n",
    "     * GKE should be enabled for that project\n",
    "  1. Change **data_dir** and **job_dir**\n",
    "     * Use a GCS bucket that you have access to\n",
    "     * Ensure the service account on your GKE cluster can read/write to this GCS bucket\n",
    "\n",
    "* Optional change the cluster name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project=\"cloud-ml-dev\"\n",
    "zone=\"us-east1-d\"\n",
    "cluster_name=\"gke-tf-example\"\n",
    "registry = \"gcr.io/\" + project\n",
    "data_dir = \"gs://cloud-ml-dev_jlewi/cifar10/data\"\n",
    "job_dirs = \"gs://cloud-ml-dev_jlewi/cifar10/jobs\"\n",
    "gke = discovery.build(\"container\", \"v1\")\n",
    "namespace = \"default\"\n",
    "\n",
    "# Whether to build containers using Google Container Builder.\n",
    "# Set to false it will build by shelling out to docker build.\n",
    "use_gcb = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GKE Cluster Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The instructions below create a **CPU** cluster\n",
    "* To create a GKE cluster with GPUs sign up for the [GKE GPU Alpha](https://goo.gl/forms/ef7eh2x00hV3hahx1)\n",
    "* To use GPUs set accelerator and accelerator_count\n",
    "* For a full list of cluster options see the [Cluster object](https://cloud.google.com/container-engine/reference/rest/v1/projects.zones.clusters#Cluster) \n",
    "  in the GKE API docs\n",
    "\n",
    "To use an existing GKE cluster call **configure_kubectl** but not **create_cluster**\n",
    "\n",
    "* The code below issues a GKE request to create the cluster by calling util.create_cluster\n",
    "  * util.create_cluster uses the GKE python client library\n",
    "* After creating the cluster we call util.configure_kubectl\n",
    "  * This configures your machine to talk to the K8s master of the newly created cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:googleapiclient.discovery:URL being requested: POST https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/clusters?alt=json\n",
      "INFO:root:Creating cluster; project=cloud-ml-dev, zone=us-east1-d, name=gke-tf-example\n",
      "INFO:root:Response {u'status': u'RUNNING', u'name': u'operation-1509663140008-3fab9123', u'zone': u'us-east1-d', u'startTime': u'2017-11-02T22:52:20.008536059Z', u'targetLink': u'https://container.googleapis.com/v1/projects/236417448818/zones/us-east1-d/clusters/gke-tf-example', u'operationType': u'CREATE_CLUSTER', u'selfLink': u'https://container.googleapis.com/v1/projects/236417448818/zones/us-east1-d/operations/operation-1509663140008-3fab9123'}\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:googleapiclient.discovery:URL being requested: GET https://container.googleapis.com/v1/projects/cloud-ml-dev/zones/us-east1-d/operations/operation-1509663140008-3fab9123?alt=json\n",
      "INFO:root:Cluster creation done.\n",
      " {u'status': u'DONE', u'name': u'operation-1509663140008-3fab9123', u'zone': u'us-east1-d', u'startTime': u'2017-11-02T22:52:20.008536059Z', u'targetLink': u'https://container.googleapis.com/v1/projects/236417448818/zones/us-east1-d/clusters/gke-tf-example', u'operationType': u'CREATE_CLUSTER', u'endTime': u'2017-11-02T22:54:35.718964976Z', u'selfLink': u'https://container.googleapis.com/v1/projects/236417448818/zones/us-east1-d/operations/operation-1509663140008-3fab9123'}\n",
      "INFO:root:Configuring kubectl\n",
      "INFO:root:Running: gcloud --project=cloud-ml-dev container clusters --zone=us-east1-d get-credentials gke-tf-example \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "WARNING: Accessing a Container Engine cluster requires the kubernetes commandline\n",
      "client [kubectl]. To install, run\n",
      "  $ gcloud components install kubectl\n",
      "\n",
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for gke-tf-example.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(util)\n",
    "machine_type = \"n1-standard-8\"\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "  accelerator = \"nvidia-tesla-k80\"\n",
    "  accelerator_count = 1\n",
    "else:\n",
    "  accelerator = None\n",
    "  accelerator_count = 0\n",
    "\n",
    "cluster_request = {\n",
    "    \"cluster\": {\n",
    "        \"name\": cluster_name,\n",
    "        \"description\": \"A GKE cluster for TF.\",\n",
    "        \"initialNodeCount\": 1,\n",
    "        \"nodeConfig\": {\n",
    "            \"machineType\": machine_type,\n",
    "            \"oauthScopes\": [\n",
    "              \"https://www.googleapis.com/auth/cloud-platform\",\n",
    "            ],\n",
    "        },\n",
    "        # TODO(jlewi): Stop pinning GKE version once 1.8 becomes the default. \n",
    "        \"initialClusterVersion\": \"1.8.1-gke.1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "if bool(accelerator) != (accelerator_count > 0):\n",
    "    raise ValueError(\"If accelerator is set accelerator_count must be  > 0\")\n",
    "    \n",
    "if accelerator:\n",
    "  # TODO(jlewi): Stop enabling Alpha once GPUs make it out of Alpha\n",
    "  cluster_request[\"cluster\"][\"enableKubernetesAlpha\"] = True\n",
    "\n",
    "  cluster_request[\"cluster\"][\"nodeConfig\"][\"accelerators\"] = [\n",
    "      {\n",
    "        \"acceleratorCount\": accelerator_count,\n",
    "        \"acceleratorType\": accelerator,\n",
    "      },\n",
    "  ]\n",
    "util.create_cluster(gke, project, zone, cluster_request)\n",
    "\n",
    "util.configure_kubectl(project, zone, cluster_name)\n",
    "\n",
    "k8s_config.load_kube_config()\n",
    "\n",
    "# Create an API client object to talk to the K8s master.\n",
    "api_client = k8s_client.ApiClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Operator\n",
    "\n",
    "* We need to deploy the [TFJob](https://github.com/kubeflow/tf-operator) custom resource on our K8s cluster\n",
    "* TFJob is deployed using the [helm](https://github.com/kubernetes/helm) package manager so first we need to setup helm on our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating service account for tiller.\n",
      "INFO:root:Service account tiller already exists.\n",
      "INFO:root:Role binding for service account tiller already exists.\n",
      "INFO:root:Running: helm init --service-account=tiller \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "$HELM_HOME has been configured at /root/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n",
      "\n",
      "INFO:root:GPUs detected in cluster.\n",
      "INFO:root:Install GPU Drivers.\n",
      "INFO:root:GPU driver daemon set has already been installed\n",
      "INFO:root:tiller is ready\n",
      "INFO:root:GPUs are available.\n"
     ]
    }
   ],
   "source": [
    "util.setup_cluster(api_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that helm is setup we can deploy the TFJob CRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running: helm install https://storage.googleapis.com/tf-on-k8s-dogfood-releases/latest/tf-job-operator-chart-latest.tgz -n tf-job --wait --replace --set rbac.install=true,cloud=gke \n",
      "cwd=None\n",
      "INFO:root:Subprocess output:\n",
      "NAME:   tf-job\n",
      "LAST DEPLOYED: Fri Nov  3 02:00:48 2017\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                             READY  STATUS   RESTARTS  AGE\n",
      "tf-job-operator-b4598cf8c-fkbc2  1/1    Running  0         2s\n",
      "\n",
      "==> v1/ConfigMap\n",
      "NAME                    DATA  AGE\n",
      "tf-job-operator-config  1     2s\n",
      "\n",
      "==> v1/ServiceAccount\n",
      "NAME             SECRETS  AGE\n",
      "tf-job-operator  1        2s\n",
      "\n",
      "==> v1beta1/ClusterRole\n",
      "NAME             AGE\n",
      "tf-job-operator  2s\n",
      "\n",
      "==> v1beta1/ClusterRoleBinding\n",
      "NAME             AGE\n",
      "tf-job-operator  2s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE\n",
      "tf-job-operator  1        1        1           1          2s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHART=\"https://storage.googleapis.com/tf-on-k8s-dogfood-releases/latest/tf-job-operator-chart-latest.tgz\"\n",
    "util.run([\"helm\", \"install\", CHART, \"-n\", \"tf-job\", \"--wait\", \"--replace\", \"--set\", \"rbac.install=true,cloud=gke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a TensorFlow program on K8s we need to package our code as Docker images.\n",
    "\n",
    "The [Dockerfile](https://github.com/jlewi/k8s/blob/73616f09f335defc92f9b20225c272862e92e32b/examples/tensorflow-models/Dockerfile.template) \n",
    "for this example starts with the published Docker images for TensorFlow ands \n",
    "the code for our TensorFlow program\n",
    "  * In this example we are using the CIFAR10 example in the [TensorFlow's model zoo](https://github.com/tensorflow/models)\n",
    "  * So our Dockerfile just clones that repo\n",
    "  * Using TF's Docker images ensures we start with a reliable TF environment \n",
    "\n",
    "We need to build separate Docker images for CPU and GPU versions of TensorFlow.\n",
    "  * **modes** controls whether we build images for CPU, GPU or both \n",
    "  * Our Dockerfile is a [Jinja2](http://jinja.pocoo.org/) template, so we can easily\n",
    "    build docker images based on different TensorFlow versions\n",
    "  \n",
    "The base images controls which version of TensorFlow we will use\n",
    "  * Change the base images if you want to use a different version.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:context_dir: /tmp/tmpTFJobSampleContentxtT189yj\n",
      "INFO:root:Running docker build -t gcr.io/cloud-ml-dev/tf-models-cpu:f67d286-dirty-9d1a089 /tmp/tmpTFJobSampleContentxtT189yj\n",
      "Sending build context to Docker daemon  5.12 kB\n",
      "INFO:root:Step 1 : FROM gcr.io/tensorflow/tensorflow:1.3.0\n",
      "INFO:root:---> 1bb38d61d261\n",
      "INFO:root:Step 2 : RUN apt-get update && apt-get install -y --no-install-recommends     ca-certificates     build-essential     git\n",
      "INFO:root:---> Using cache\n",
      "INFO:root:---> 02d9bcdd5293\n",
      "INFO:root:Step 3 : RUN git clone https://github.com/jlewi/models.git /tensorflow_models &&     cd /tensorflow_models &&     git checkout generate_records\n",
      "INFO:root:---> Using cache\n",
      "INFO:root:---> e1d25a2ebd6c\n",
      "INFO:root:Successfully built e1d25a2ebd6c\n",
      "INFO:root:Built image: gcr.io/cloud-ml-dev/tf-models-cpu:f67d286-dirty-9d1a089\n",
      "INFO:root:Running gcloud docker -- push gcr.io/cloud-ml-dev/tf-models-cpu:f67d286-dirty-9d1a089\n",
      "INFO:root:The push refers to a repository [gcr.io/cloud-ml-dev/tf-models-cpu]\n",
      "INFO:root:85519424f2bc: Preparing\n",
      "INFO:root:9174b4aa0015: Preparing\n",
      "INFO:root:cca7884663e6: Preparing\n",
      "INFO:root:c9c04a5fd1a3: Preparing\n",
      "INFO:root:5d4dbb0c7791: Preparing\n",
      "INFO:root:6a19be88e574: Preparing\n",
      "INFO:root:adcfc17fe4eb: Preparing\n",
      "INFO:root:8f196722f8c6: Preparing\n",
      "INFO:root:eac59d81aaf0: Preparing\n",
      "INFO:root:a09947e71dc0: Preparing\n",
      "INFO:root:9c42c2077cde: Preparing\n",
      "INFO:root:625c7a2a783b: Preparing\n",
      "INFO:root:25e0901a71b8: Preparing\n",
      "INFO:root:8aa4fcad5eeb: Preparing\n",
      "INFO:root:8f196722f8c6: Waiting\n",
      "INFO:root:eac59d81aaf0: Waiting\n",
      "INFO:root:a09947e71dc0: Waiting\n",
      "INFO:root:9c42c2077cde: Waiting\n",
      "INFO:root:625c7a2a783b: Waiting\n",
      "INFO:root:25e0901a71b8: Waiting\n",
      "INFO:root:8aa4fcad5eeb: Waiting\n",
      "INFO:root:6a19be88e574: Waiting\n",
      "INFO:root:adcfc17fe4eb: Waiting\n",
      "INFO:root:85519424f2bc: Layer already exists\n",
      "INFO:root:9174b4aa0015: Layer already exists\n",
      "INFO:root:c9c04a5fd1a3: Layer already exists\n",
      "INFO:root:cca7884663e6: Layer already exists\n",
      "INFO:root:5d4dbb0c7791: Layer already exists\n",
      "INFO:root:a09947e71dc0: Layer already exists\n",
      "INFO:root:eac59d81aaf0: Layer already exists\n",
      "INFO:root:adcfc17fe4eb: Layer already exists\n",
      "INFO:root:6a19be88e574: Layer already exists\n",
      "INFO:root:8f196722f8c6: Layer already exists\n",
      "INFO:root:9c42c2077cde: Layer already exists\n",
      "INFO:root:8aa4fcad5eeb: Layer already exists\n",
      "INFO:root:625c7a2a783b: Layer already exists\n",
      "INFO:root:25e0901a71b8: Layer already exists\n",
      "INFO:root:f67d286-dirty-9d1a089: digest: sha256:76f4ac249c44d8cc05c580c8849da0c88971fae10e8e0e412ebac5784c1bd60f size: 3255\n",
      "INFO:root:\n",
      "INFO:root:Pushed image: gcr.io/cloud-ml-dev/tf-models-cpu:f67d286-dirty-9d1a089\n",
      "INFO:root:context_dir: /tmp/tmpTFJobSampleContentxtIQHMeQ\n",
      "INFO:root:Running docker build -t gcr.io/cloud-ml-dev/tf-models-gpu:f67d286-dirty-9d1a089 /tmp/tmpTFJobSampleContentxtIQHMeQ\n",
      "Sending build context to Docker daemon  5.12 kB\n",
      "INFO:root:Step 1 : FROM gcr.io/tensorflow/tensorflow:1.3.0-gpu\n",
      "INFO:root:---> 4ca1d30fcd9b\n",
      "INFO:root:Step 2 : RUN apt-get update && apt-get install -y --no-install-recommends     ca-certificates     build-essential     git\n",
      "INFO:root:---> Using cache\n",
      "INFO:root:---> 8ebf15bb2cc0\n",
      "INFO:root:Step 3 : RUN git clone https://github.com/jlewi/models.git /tensorflow_models &&     cd /tensorflow_models &&     git checkout generate_records\n",
      "INFO:root:---> Using cache\n",
      "INFO:root:---> 7857fac49b94\n",
      "INFO:root:Successfully built 7857fac49b94\n",
      "INFO:root:Built image: gcr.io/cloud-ml-dev/tf-models-gpu:f67d286-dirty-9d1a089\n",
      "INFO:root:Running gcloud docker -- push gcr.io/cloud-ml-dev/tf-models-gpu:f67d286-dirty-9d1a089\n",
      "INFO:root:The push refers to a repository [gcr.io/cloud-ml-dev/tf-models-gpu]\n",
      "INFO:root:6299bb1d50ec: Preparing\n",
      "INFO:root:8b32f5caac4f: Preparing\n",
      "INFO:root:d0370c4745c2: Preparing\n",
      "INFO:root:3f5724c08688: Preparing\n",
      "INFO:root:b047af883cd3: Preparing\n",
      "INFO:root:d7d47b9fad98: Preparing\n",
      "INFO:root:158542675315: Preparing\n",
      "INFO:root:0a78661b279e: Preparing\n",
      "INFO:root:b03b4e1bca4c: Preparing\n",
      "INFO:root:e30fb6933f61: Preparing\n",
      "INFO:root:c1aa6c8b1868: Preparing\n",
      "INFO:root:4a4139031328: Preparing\n",
      "INFO:root:826d415e0c33: Preparing\n",
      "INFO:root:aee2d05a4618: Preparing\n",
      "INFO:root:daf2a8643391: Preparing\n",
      "INFO:root:533c6bb6b046: Preparing\n",
      "INFO:root:a09947e71dc0: Preparing\n",
      "INFO:root:9c42c2077cde: Preparing\n",
      "INFO:root:625c7a2a783b: Preparing\n",
      "INFO:root:25e0901a71b8: Preparing\n",
      "INFO:root:8aa4fcad5eeb: Preparing\n",
      "INFO:root:4a4139031328: Waiting\n",
      "INFO:root:826d415e0c33: Waiting\n",
      "INFO:root:a09947e71dc0: Waiting\n",
      "INFO:root:aee2d05a4618: Waiting\n",
      "INFO:root:9c42c2077cde: Waiting\n",
      "INFO:root:daf2a8643391: Waiting\n",
      "INFO:root:625c7a2a783b: Waiting\n",
      "INFO:root:533c6bb6b046: Waiting\n",
      "INFO:root:25e0901a71b8: Waiting\n",
      "INFO:root:8aa4fcad5eeb: Waiting\n",
      "INFO:root:158542675315: Waiting\n",
      "INFO:root:b03b4e1bca4c: Waiting\n",
      "INFO:root:0a78661b279e: Waiting\n",
      "INFO:root:e30fb6933f61: Waiting\n",
      "INFO:root:c1aa6c8b1868: Waiting\n",
      "INFO:root:d7d47b9fad98: Waiting\n",
      "INFO:root:8b32f5caac4f: Layer already exists\n",
      "INFO:root:3f5724c08688: Layer already exists\n",
      "INFO:root:d0370c4745c2: Layer already exists\n",
      "INFO:root:b047af883cd3: Layer already exists\n",
      "INFO:root:6299bb1d50ec: Layer already exists\n",
      "INFO:root:b03b4e1bca4c: Layer already exists\n",
      "INFO:root:158542675315: Layer already exists\n",
      "INFO:root:d7d47b9fad98: Layer already exists\n",
      "INFO:root:0a78661b279e: Layer already exists\n",
      "INFO:root:e30fb6933f61: Layer already exists\n",
      "INFO:root:c1aa6c8b1868: Layer already exists\n",
      "INFO:root:826d415e0c33: Layer already exists\n",
      "INFO:root:aee2d05a4618: Layer already exists\n",
      "INFO:root:4a4139031328: Layer already exists\n",
      "INFO:root:daf2a8643391: Layer already exists\n",
      "INFO:root:533c6bb6b046: Layer already exists\n",
      "INFO:root:9c42c2077cde: Layer already exists\n",
      "INFO:root:a09947e71dc0: Layer already exists\n",
      "INFO:root:625c7a2a783b: Layer already exists\n",
      "INFO:root:25e0901a71b8: Layer already exists\n",
      "INFO:root:8aa4fcad5eeb: Layer already exists\n",
      "INFO:root:f67d286-dirty-9d1a089: digest: sha256:91d89c106f77baf91ad965933f4479f2e08428bb73a0a879d615baf11d2f4fd2 size: 4726\n",
      "INFO:root:\n",
      "INFO:root:Pushed image: gcr.io/cloud-ml-dev/tf-models-gpu:f67d286-dirty-9d1a089\n"
     ]
    }
   ],
   "source": [
    "reload(build_and_push_image)\n",
    "\n",
    "if use_gpu:\n",
    "  modes = [\"cpu\", \"gpu\"]\n",
    "else:\n",
    "  modes = [\"cpu\"]\n",
    "\n",
    "image = os.path.join(registry, \"tf-models\")\n",
    "dockerfile = os.path.join(ROOT_DIR, \"examples\", \"tensorflow-models\", \"Dockerfile.template\")\n",
    "base_images = {\n",
    "  \"cpu\": \"gcr.io/tensorflow/tensorflow:1.3.0\",\n",
    "  \"gpu\": \"gcr.io/tensorflow/tensorflow:1.3.0-gpu\",\n",
    "}\n",
    "images = build_and_push_image.build_and_push(dockerfile, image, modes=modes, base_images=base_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CIFAR10 Datasets\n",
    "\n",
    "We need to create the cifar10 TFRecord files by running [generate_cifar10_tfrecords.py](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py)\n",
    "  * We submit a K8s job to run this program\n",
    "  * You can skip this step if your data is already available in data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created job cifar10-data-171028-012554\n"
     ]
    }
   ],
   "source": [
    "batch_api = k8s_client.BatchV1Api(api_client)\n",
    "\n",
    "job_name = \"cifar10-data-\"+ datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "body = {}\n",
    "body['apiVersion'] = \"batch/v1\"\n",
    "body['kind'] = \"Job\"\n",
    "body['metadata'] = {}\n",
    "body['metadata']['name'] = job_name\n",
    "body['metadata']['namespace'] = namespace\n",
    "\n",
    "# Note backoffLimit requires K8s >= 1.8\n",
    "spec = \"\"\"\n",
    "backoffLimit: 4\n",
    "template:\n",
    "  spec:\n",
    "    containers:\n",
    "    - name: cifar10\n",
    "      image: {image}\n",
    "      command: [\"python\",  \"/tensorflow_models/tutorials/image/cifar10_estimator/generate_cifar10_tfrecords.py\", \"--data-dir={data_dir}\"]\n",
    "    restartPolicy: Never\n",
    "\"\"\".format(data_dir=data_dir, image=images[\"cpu\"])\n",
    "\n",
    "spec_buffer = StringIO.StringIO(spec)\n",
    "body['spec'] = yaml.load(spec_buffer)\n",
    "\n",
    "try: \n",
    "    # Create a Resource\n",
    "    api_response = batch_api.create_namespaced_job(namespace, body)\n",
    "    print(\"Created job %s\" % api_response.metadata.name)\n",
    "except ApiException as e:\n",
    "    print(\n",
    "        \"Exception when calling DefaultApi->apis_fqdn_v1_namespaces_namespace_resource_post: %s\\n\" % \n",
    "        e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wait for the job to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Waiting for job cifar10-data-171028-012554 ....\n",
      "Job completed successfully\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  results = batch_api.read_namespaced_job(job_name, namespace)\n",
    "  if results.status.succeeded >= 1 or results.status.failed >= 3:\n",
    "    break\n",
    "  print(\"Waiting for job %s ....\" % results.metadata.name)\n",
    "  time.sleep(5)\n",
    "\n",
    "if results.status.succeeded >= 1:\n",
    "  print(\"Job completed successfully\")\n",
    "else:\n",
    "  print(\"Job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TFJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit a TFJob, we define a TFJob spec and then create it in our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): accounts.google.com\n",
      "INFO:root:Created job cifar10-171103-030034\n"
     ]
    }
   ],
   "source": [
    "crd_api = k8s_client.CustomObjectsApi(api_client)\n",
    "\n",
    "namespace = \"default\"\n",
    "job_name = \"cifar10-\"+ datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "job_dir = os.path.join(job_dirs, job_name)\n",
    "num_steps = 10\n",
    "body = {}\n",
    "body['apiVersion'] = TF_JOB_GROUP + \"/\" + TF_JOB_VERSION\n",
    "body['kind'] = TF_JOB_KIND\n",
    "body['metadata'] = {}\n",
    "body['metadata']['name'] = job_name\n",
    "body['metadata']['namespace'] = namespace\n",
    "\n",
    "master_image = images[\"cpu\"]\n",
    "if use_gpu:\n",
    "  master_image = images[\"gpu\"]\n",
    "spec = \"\"\"\n",
    "  replicaSpecs:\n",
    "    - replicas: 1\n",
    "      tfReplicaType: MASTER\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "            - image: {master_image}\n",
    "              name: tensorflow\n",
    "              command:\n",
    "                - python\n",
    "                - /tensorflow_models/tutorials/image/cifar10_estimator/cifar10_main.py\n",
    "                - --data-dir={data_dir}\n",
    "                - --job-dir={job_dir}\n",
    "                - --train-steps={num_steps}\n",
    "                - --num-gpus={num_gpus}\n",
    "          restartPolicy: OnFailure\n",
    "  tfImage: {cpu_image}\n",
    "  tensorBoard:\n",
    "    logDir: {job_dir}\n",
    "\"\"\".format(master_image=master_image, cpu_image=images[\"cpu\"], data_dir=data_dir, job_dir=job_dir, num_steps=num_steps, num_gpus=accelerator_count)\n",
    "\n",
    "spec_buffer = StringIO.StringIO(spec)\n",
    "body['spec'] = yaml.load(spec_buffer)\n",
    "if use_gpu:\n",
    "  body['spec']['replicaSpecs'][0][\"template\"][\"spec\"][\"containers\"][0][\"resources\"] = {\n",
    "    \"limits\": {\n",
    "      \"nvidia.com/gpu\": accelerator_count,\n",
    "    }    \n",
    "  }\n",
    "\n",
    "try: \n",
    "    # Create a Resource\n",
    "    api_response = crd_api.create_namespaced_custom_object(TF_JOB_GROUP, TF_JOB_VERSION, namespace, TF_JOB_PLURAL, body) \n",
    "    logging.info(\"Created job %s\", api_response[\"metadata\"][\"name\"])\n",
    "except ApiException as e:\n",
    "    print(\n",
    "        \"Exception when calling DefaultApi->apis_fqdn_v1_namespaces_namespace_resource_post: %s\\n\" % \n",
    "        e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring your job and waiting for it to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the job a number of ways\n",
    "  * We can poll K8s to get the status of the TFJob\n",
    "  * We can check the TensorFlow logs\n",
    "      * These are available in StackDriver\n",
    "  * We can access TensorBoard if the TFJob was configured to launch TensorBoard\n",
    "  \n",
    "Running the code below will poll K8s for the TFJob status and also print out relevant links for TensorBoard and the StackDriver logs\n",
    "\n",
    "To access TensorBoard you will need to run **kubectl proxy** to create a proxy connection to your K8s cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Job has runtime id: eg5k\n",
      "INFO:root:Tensorboard will be available at job\n",
      " http://127.0.0.1:8001/api/v1/proxy/namespaces/default/services/tensorboard-eg5k:80/\n",
      "INFO:root:master pod is master-eg5k-0-p85pk\n",
      "INFO:root:Logs will be available in stackdriver at\n",
      "https://console.cloud.google.com/logs/viewer?expandAll=false&dateRangeStart=2017-11-03T03%3A00%3A43%2B00%3A00&advancedFilter=resource.type%3D%22container%22%0Aresource.labels.namespace_id%3D%22default%22%0Aresource.labels.pod_id%3D%22master-eg5k-0-p85pk%22&interval=NO_LIMIT&project=cloud-ml-dev&logName=projects%2Fcloud-ml-dev%2Flogs%2Ftensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n",
      "Job status Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Job Succeeded\n"
     ]
    }
   ],
   "source": [
    "# Get pod logs\n",
    "v1 = k8s_client.CoreV1Api(api_client)\n",
    "\n",
    "k8s_config.load_kube_config()\n",
    "api_client = k8s_client.ApiClient()\n",
    "crd_api = k8s_client.CustomObjectsApi(api_client)\n",
    "\n",
    "master_started = False\n",
    "runtime_id = None\n",
    "while True:\n",
    "  results = crd_api.get_namespaced_custom_object(TF_JOB_GROUP, TF_JOB_VERSION, namespace, TF_JOB_PLURAL, job_name)\n",
    "\n",
    "  if not runtime_id:\n",
    "    runtime_id = results[\"spec\"][\"RuntimeId\"]\n",
    "    logging.info(\"Job has runtime id: %s\", runtime_id)\n",
    "    \n",
    "    tensorboard_url = \"http://127.0.0.1:8001/api/v1/proxy/namespaces/{namespace}/services/tensorboard-{runtime_id}:80/\".format(\n",
    "    namespace=namespace, runtime_id=runtime_id)\n",
    "    logging.info(\"Tensorboard will be available at job\\n %s\", tensorboard_url)\n",
    "\n",
    "  if not master_started:\n",
    "    # Get the master pod\n",
    "    # TODO(jlewi): V1LabelSelector doesn't seem to help\n",
    "    pods = v1.list_namespaced_pod(namespace=namespace, label_selector=\"runtime_id={0},job_type=MASTER\".format(runtime_id))\n",
    "\n",
    "    # TODO(jlewi): We should probably handle the case where more than 1 pod gets started.\n",
    "    # TODO(jlewi): Once GKE logs pod labels we can just filter by labels to get all logs for a particular task\n",
    "    # and not have to identify the actual pod.\n",
    "    if pods.items:\n",
    "      pod = pods.items[0]\n",
    "\n",
    "      logging.info(\"master pod is %s\", pod.metadata.name)\n",
    "      query={\n",
    "        'advancedFilter': 'resource.type=\"container\"\\nresource.labels.namespace_id=\"default\"\\nresource.labels.pod_id=\"{0}\"'.format(pod.metadata.name), \n",
    "        'dateRangeStart': pod.metadata.creation_timestamp.isoformat(),\n",
    "        'expandAll': 'false',\n",
    "        'interval': 'NO_LIMIT',\n",
    "        'logName': 'projects/{0}/logs/tensorflow'.format(project),\n",
    "       'project': project, \n",
    "      }\n",
    "      logging.info(\"Logs will be available in stackdriver at\\n\"\n",
    "                   \"https://console.cloud.google.com/logs/viewer?\" + urllib.urlencode(query))\n",
    "      master_started = True\n",
    "\n",
    "  if results[\"status\"][\"phase\"] == \"Done\":\n",
    "    break\n",
    "  print(\"Job status {0}\".format(results[\"status\"][\"phase\"]))\n",
    "  time.sleep(5)\n",
    "  \n",
    "logging.info(\"Job %s\", results[\"status\"][\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "* Delete the GKE cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function delete_cluster in module py.util:\n",
      "\n",
      "delete_cluster(gke, name, project, zone)\n",
      "    Delete the cluster.\n",
      "    \n",
      "    Args:\n",
      "      gke: Client for GKE.\n",
      "      name: Name of the cluster.\n",
      "      project: Project that owns the cluster.\n",
      "      zone: Zone where the cluster is running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "util.delete_cluster(gke, cluster_name, project, zone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): accounts.google.com\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9187b38024ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapi_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApiClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk8s_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoreV1Api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mruntime_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"RuntimeId\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO(jlewi): V1LabelSelector doesn't seem to help\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_namespaced_pod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_selector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"runtime_id={0},job_type=MASTER\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "from kubernetes.client.models.v1_label_selector import V1LabelSelector\n",
    "import urllib2\n",
    "# Get pod logs\n",
    "k8s_config.load_kube_config()\n",
    "api_client = k8s_client.ApiClient()\n",
    "v1 = k8s_client.CoreV1Api(api_client)\n",
    "runtime_id = results[\"spec\"][\"RuntimeId\"]\n",
    "# TODO(jlewi): V1LabelSelector doesn't seem to help\n",
    "pods = v1.list_namespaced_pod(namespace=namespace, label_selector=\"runtime_id={0},job_type=MASTER\".format(runtime_id))\n",
    "\n",
    "pod = pods.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the Pod Logs From K8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read pod logs directly from K8s and not depend on stackdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-27 23:37:29,807 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', error(104, 'Connection reset by peer'))': /api/v1/namespaces/default/pods/master-hrhh-0-wrh6g/log\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', error(104, 'Connection reset by peer'))': /api/v1/namespaces/default/pods/master-hrhh-0-wrh6g/log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': gpu_options {\n",
      "  force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb715c9b10>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-10-27 17:42:51.299775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299827: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299834: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.74784, step = 1\n",
      "INFO:tensorflow:loss = 4.74784, learning_rate = 0.1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.2765.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-27-17:44:06\n",
      "INFO:tensorflow:Restoring parameters from gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt-10\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-27-17:45:24\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.1, global_step = 10, loss = 2.26738e+20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ret = v1.read_namespaced_pod_log(namespace=namespace, name=pod.metadata.name)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch Logs from StackDriver Programmatically\n",
    "  * On GKE pod logs are stored in stackdriver\n",
    "  * These logs will stick around longer than pod logs\n",
    "  * Fetching from stackNote this tends to be a little slow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth._default:No project ID could be determined from the Cloud SDK configuration. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': gpu_options {\n",
      "force_gpu_compatible: true\n",
      "}\n",
      "allow_soft_placement: true\n",
      ", '_tf_random_seed': None, '_task_type': u'master', '_environment': u'cloud', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fcb715c9b10>, '_tf_config': gpu_options {\n",
      "per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "2017-10-27 17:42:51.299775: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299827: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299834: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299839: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-27 17:42:51.299845: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.74784, step = 1\n",
      "INFO:tensorflow:loss = 4.74784, learning_rate = 0.1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 11.2765.\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_1/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_2/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_3/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_4/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_5/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage/residual_v1_6/: (?, 32, 32, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/avg_pool/: (?, 16, 16, 16)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_1/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_2/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_3/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_4/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_5/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_1/residual_v1_6/: (?, 16, 16, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/avg_pool/: (?, 8, 8, 32)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_1/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_2/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_3/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_4/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_5/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/stage_2/residual_v1_6/: (?, 8, 8, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/global_avg_pool/: (?, 64)\n",
      "INFO:tensorflow:image after unit resnet/tower_0/fully_connected/: (?, 11)\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-27-17:44:06\n",
      "INFO:tensorflow:Restoring parameters from gs://cloud-ml-dev_jlewi/cifar10/jobs/cifar10-171027-174224/model.ckpt-10\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-27-17:45:24\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.1, global_step = 10, loss = 2.26738e+20\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import logging as gcp_logging\n",
    "pod_filter = 'resource.type=\"container\" AND resource.labels.pod_id=\"master-hrhh-0-wrh6g\"'\n",
    "client = gcp_logging.Client(project=project)\n",
    "\n",
    "for entry in client.list_entries(filter_=pod_filter):\n",
    "  print(entry.payload.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
